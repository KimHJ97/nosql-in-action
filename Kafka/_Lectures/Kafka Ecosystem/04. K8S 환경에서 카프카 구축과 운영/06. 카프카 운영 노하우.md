# 카프카 운영 노하우

## 1. 파티션 개수 변경

### 1-1. 토픽 파티션

 - 하나의 토픽은 1개 이상의 파티션으로 구성된다.
 - 파티션은 실제 스토리지에 directory 형태로 생성되며 디렉토리 내부에는 토픽 파티션과 연관된 여러 파일이 생성된다.

### 1-2. 파티션 개수

 - 파티션의 개수는 브로커의 수량과 관련이 없다.
    - 클러스터에 파티션의 수가 증가하면 메타데이터도 증가하여 성능에 영향을 줄 수 있다.
    - 적당한 수의 파티션을 생성하여 사용하는 것을 권장한다.
 - __토픽의 파티션 개수는 컨슈머 그룹의 동시성 상한선관 관련된다.__
    - __하나의 파티션은 같은 컨슈머 그룹 내 하나의 컨슈머에만 할당 가능하다.__
    - 파티션 개수 = 동일 컨슈머 그룹에서 동시에 메시지를 처리할 수 있는 최대 컨슈머 수량
 - 파티션 개수는 프로듀서 혹은 컨슈머의 연결 상태와 관계없이 증가 시키는 것이 가능하다.
    - __파티션 개수를 줄이는 것은 불가능하므로 늘리는 것을 신중하게 결정해야 한다.__

### 1-3. 최대 파티션 개수

 - 주키퍼 클러스터의 경우
    - 하나의 카프카 클러스터는 20만 개 정도의 파티션을 지원 가능
    - 각 브로커는 약 4,000개의 파티션을 지원 가능
    - 토픽의 모든 파티션 복제본을 개별로 카운트
 - 더 많은 파티션은 더 많은 자원을 소모한다.
    - 동시에 open 해야 하는 파일의 개수 증가 -> 더 많은 OS 자원 소모
    - 관리해야 하는 메타데이터 양 증가 -> 프로듀서 메모리 소모량 증가
    - 장애시 리더 선출 시간 증가
 - KRaft 클러스터의 경우 더 많은 파티션을 지원할 수 있다.
    - 그럼에도 관리해야 하는 메타데이터 양은 동일하다.

### 1-4. 적절한 파티션 개수

 - 각 브로커 스토리지 사용량의 균형을 유지하기에 용이하도록 브로커 개수의 배수로 파티션 개수를 설정하는 것이 좋다.
    - 예시: 브로커 개수 4개 -> 파티션 4개 / 8개 / 12개 / ..
    - 토픽 전체 메시지에 대한 순서 보장이 필요한 경우 파티션은 반드시 1개
    - 메시지에 사용하는 키 개수가 정해져 있고 각 키를 별도의 파티션으로 관리하고자 하는 경우 파티션은 키의 개수만큼 생성
    - 프로듀서에서 메시지를 분류하는 명확한 기준이 있고, 기준에 따라 분류된 메시지를 동일 파티션에서 관리하고자 하는 경우
 - 파티션 개수는 토픽을 구독하는 컨슈머의 최대 동시성을 제한하므로, 반드시 동일 컨슈머 그룹의 최대 동시성 개수 이상으로 설정해야 한다.
    - 브로커 개수의 배수로 설정 조건과 동시 적용할 수 있다.
    - 예시: 브로커 개수 3개, 컨슈머 그룹 최대 동시성 10개 -> 파티션 개수 12개

#### 1-5. 파티션 개수 변경

 - `파티션 개수 변경시 영향`
    - 메시지 키를 사용하는 경우 프로듀서의 파티셔너에 따라 메시지가 보내지는 파티션이 변경될 수 있음
    - 파티션 변경은 즉시 반영되지 않고 메타데이터가 갱신된 이후 반영됨

```bash
# 현재 토픽 구성 확인
/opt/bitnami/kafka/bin/kafka-topics.sh \
--bootstrap-server my-kafka-headless:9092 \
--command-config /tmp/devops-client.properties \
--describe --topic test-datagen-orders

# 테스트 플로듀서 설정 변경
cat > test-producer-configmap.yaml <<EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: test-producer-config
data:
  producer-client.properties: |
    security.protocol=SASL_PLAINTEXT
    sasl.mechanism=SCRAM-SHA-512
    sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required \
    username="producer" password="producer-secret";
    metadata.max.age.ms=30000

EOF

kubectl rollout restart deployment/test-producer

# 테스트 프로듀서 재시작
kubectl rollout restart deployment/test-producer

# 테스트 컨슈머 개수 및 설정 변경
cat > test-consumer-deployment.yaml <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    spitha.io: test-consumer
  name: test-consumer
spec:
  replicas: 4
  selector:
    matchLabels:
      spitha.io: test-consumer
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        spitha.io: test-consumer
    spec:
      containers:
      - image: docker.io/bitnami/kafka:3.3.1-debian-11-r19
        imagePullPolicy: ""
        name: test-consumer
        command:
        - "/opt/bitnami/kafka/bin/kafka-console-consumer.sh"
        args:
        - --bootstrap-server
        - my-kafka-headless:9092
        - --group
        - test-consumer-group
        - --topic
        - test-datagen-orders
        - --consumer-property
        - security.protocol=SASL_PLAINTEXT
        - --consumer-property
        - sasl.mechanism=SCRAM-SHA-512
        - --consumer-property
        - sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username="consumer" password="consumer-secret";
        - --consumer-property
        - metadata.max.age.ms=30000
      restartPolicy: Always
EOF

kubectl apply -f test-consumer-deployment.yaml

# 변경 사항 모니터링
kubectl logs -f test-consumer-xxx

# 파티션 개수 변경
/opt/bitnami/kafka/bin/kafka-topics.sh \
--bootstrap-server my-kafka-headless:9092 \
--command-config /tmp/devops-client.properties \
--topic test-datagen-orders \
--alter --partitions 4
```

## 2. Partition Reassignment

### 2-1. 파티션 재할당이 필요한 이유

 - 토픽을 생성할 떄 각 카프카 브로커에게 비슷한 개수의 파티션을 할당
 - 각 파티션은 사용량이 매우 다를 수 있다.
    - 사용량이 많은 파티션은 네트워크 사용량이 많다.
    - 파티션의 쓰기 및 읽기는 리더 파티션에 집중된다.
    - 보관 기간에 따라 다르지만, 스토리지 공간을 더 많이 사용할 확률이 높다.
 - 카프카 클러스터를 운영하다보면 브로커 사이에 스토리지 사용량이 달라질 확률이 매우 높다.
 - 브로커는 스토리지가 모자라면 즉시 중지된다.
    - 복구 절차가 상당히 까다로울 수 있으며, 복제본이 적절하게 구성되어 있지 않으면 데이터 유실 가능성이 크다.

### 2-2. 파티션을 적절하게 재할당하면

 - 스토리지 사용량을 균등하게 배치
 - 네트워크 사용량도 어느 정도 균형있게 분산시킬 수 있다.
 - `파티션을 이동할 수 없는 경우`
    - 토픽의 복제본 개수와 브로커 수가 동일한 경우
    - 동일한 토픽 파티션을 같은 브로커로 배치할 수 없다.
 - `파티션 이동 시 주의 사항`
    - 이동하고자 하는 브로커의 스토리지 여유 공간을 확인해야 한다.
    - 용량이 큰 파티션을 이동하는 경우 네트워크 샤용량이 많아져 서비스에 영향을 줄 수 있따.
        - 파티션 이동에 네트워크 로틀 설정을 추가할 수 있다.
        - 되도록 트래픽이 적은 시간대를 활용해 재할당 작업을 하는 것이 좋다.

### 2-3. 파티션 재할당하기

```bash
# 재할당 테스트 토픽 생성
/opt/bitnami/kafka/bin/kafka-topics.sh \
--bootstrap-server my-kafka-headless:9092 \
--command-config /tmp/devops-client.properties \
--create --topic reassign-test-topic --partitions 3 \
--config retention.ms=3000000 --config segment.ms=1200000

# 테스트 프로듀서 배포
cat > test-consumer-deployment.yaml <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    spitha.io: test-consumer
  name: test-consumer
spec:
  replicas: 4
  selector:
    matchLabels:
      spitha.io: test-consumer
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        spitha.io: test-consumer
    spec:
      containers:
      - image: docker.io/bitnami/kafka:3.3.1-debian-11-r19
        imagePullPolicy: ""
        name: test-consumer
        command:
        - "/opt/bitnami/kafka/bin/kafka-console-consumer.sh"
        args:
        - --bootstrap-server
        - my-kafka-headless:9092
        - --group
        - test-consumer-group
        - --topic
        - test-datagen-orders
        - --consumer-property
        - security.protocol=SASL_PLAINTEXT
        - --consumer-property
        - sasl.mechanism=SCRAM-SHA-512
        - --consumer-property
        - sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username="consumer" password="consumer-secret";
        - --consumer-property
        - metadata.max.age.ms=30000
      restartPolicy: Always
EOF

kubectl apply -f reassign-producer-deployment.yaml

# 파티션 배치 상태 확인
/opt/bitnami/kafka/bin/kafka-topics.sh \
--bootstrap-server my-kafka-headless:9092 \
--command-config /tmp/devops-client.properties \
--describe --topic reassign-test-topic

# 파티션 크기 확인
du -sh /bitnami/kafka/data/reassign-test*

# 파티션 재할당 구성 파일
cat > /tmp/reassign-partition-1.json <<EOF
{
   "partitions":[
      { "topic":"reassign-test-topic",
        "partition":0, 
        "replicas":[1]
      }
   ],
   "version":1
}
EOF

# 토픽 모니터링
/opt/bitnami/kafka/bin/kafka-console-consumer.sh \
--bootstrap-server my-kafka-headless:9092 \
--consumer.config /tmp/devops-client.properties \
--topic reassign-test-topic

# 파티션 재할당 실행
/opt/bitnami/kafka/bin/kafka-reassign-partitions.sh \
--bootstrap-server my-kafka-headless:9092 \
--command-config /tmp/devops-client.properties \
--reassignment-json-file /tmp/reassign-partition-1.json \
--execute

# 파티션 재할당 상태 확인
/opt/bitnami/kafka/bin/kafka-reassign-partitions.sh \
--bootstrap-server my-kafka-headless:9092 \
--command-config /tmp/devops-client.properties \
--reassignment-json-file /tmp/reassign-partition-1.json \
--verify

# 파티션 배치 확인
/opt/bitnami/kafka/bin/kafka-topics.sh \
--bootstrap-server my-kafka-headless:9092 \
--command-config /tmp/devops-client.properties \
--describe --topic reassign-test-topic
```

 - `스로틀링`
```bash
# 파티션 재할당 구성
cat > /tmp/reassign-partition-2.json <<EOF
{
   "partitions":[
      { "topic":"reassign-test-topic",
        "partition":0, 
        "replicas":[2] 
      }
   ],
   "version":1
}
EOF

## 스로틀 설정하여 재할당 실행
/opt/bitnami/kafka/bin/kafka-reassign-partitions.sh \
--bootstrap-server my-kafka-headless:9092 \
--command-config /tmp/devops-client.properties \
--reassignment-json-file /tmp/reassign-partition-2.json \
--execute \
--throttle 1000

# 재할당 상태 확인
/opt/bitnami/kafka/bin/kafka-reassign-partitions.sh \
--bootstrap-server my-kafka-headless:9092 \
--command-config /tmp/devops-client.properties \
--reassignment-json-file /tmp/reassign-partition-2.json \
--verify

# 스로틀 변경
/opt/bitnami/kafka/bin/kafka-reassign-partitions.sh \
--bootstrap-server my-kafka-headless:9092 \
--command-config /tmp/devops-client.properties \
--reassignment-json-file /tmp/reassign-partition-2.json \
--additional \
--execute \
--throttle 100000
```
