# Best Practices

## 1. Fail Point Disk 장애

Kafka는 데이터를 디스크에 저장하고 로그 기반 구조로 운영됩니다. 따라서 디스크 장애는 직접적인 데이터 유실, 성능 저하, 브로커 다운으로 이어질 수 있습니다.

 - __장애 유형__
    - 디스크 용량 부족 → 브로커 쓰기 중단
    - 디스크 I/O 지연 → 프로듀서/컨슈머 지연
    - 물리적 장애 → 파티션 손실, 리플리케이션 실패
 - __대응 방안__
    - log.dirs에 다중 디스크 지정 → 장애 시 분산 저장
    - 모니터링 도구(Prometheus, Grafana)로 디스크 사용량/에러 실시간 확인
    - min.insync.replicas 설정으로 가용성 보장
    - 데이터 유실 방지를 위한 Replication Factor ≥ 3 권장

## 2. Fail Point 메시지 불균형

Kafka는 파티션 단위로 메시지를 분산하지만, 특정 파티션에만 메시지가 몰릴 경우 컨슈머 부하 불균형이나 스케일링 실패가 발생할 수 있습니다.

 - __원인__
    - 키 기반 분배 사용 시 일부 키에 쏠림
    - 잘못된 파티셔닝 전략
    - 컨슈머 그룹의 리밸런싱 실패
 - __해결책__
    - Round-robin 파티셔너 또는 커스텀 파티셔너 적용
    - 컨슈머 스레드 수 조정, max.poll.records 튜닝
    - 파티션 수 재설계 (ex: 토픽 확장)
    - 컨슈머 지표 (Lag 등) 모니터링 → Burrow 활용

## 3. Fail Point IDC간 네트워크 단절

Kafka를 멀티 IDC 혹은 멀티 리전으로 운영할 경우, 네트워크 단절은 데이터 일관성, 복제 중단, 컨슈머 장애 등을 유발할 수 있습니다.

 - __주요 영향__
    - Follower 파티션 → ISR 이탈
    - Controller Failover
    - MirrorMaker 기반 복제 실패
    - 데이터 유실 우려 (acks=1 사용 시)
 - __대응 방안__
    - min.insync.replicas 보장 + acks=all 설정
    - MirrorMaker2 + Replication Lag 모니터링
    - unclean.leader.election.enable=false 설정으로 안전성 확보
    - IDC간 QoS 기반 네트워크 모니터링 체계 도입

## 4. Kafka Trend 및 Streaming Service 동향

 - __KRaft 모드__: ZooKeeper 없는 Kafka. 클러스터 관리 단순화, 고가용성 강화 (Kafka 3.x 이후)
 - __ksqlDB / SQL on Kafka__: Kafka Streams를 SQL로 추상화하여 비개발자도 스트리밍 처리가능
 - __Apache Flink 통합__: Flink + Kafka 조합이 실시간 분석 표준처럼 확산
 - __Serverless Kafka (Cloud-Native)__:" Confluent Cloud, MSK Serverless 등 → Kafka의 관리형 서비스 사용 증가
 - __CDC 기반 스트리밍 아키텍처__: Debezium, Kafka Connect를 통한 Change Data Capture 활성화
 - __AI/ML 데이터 파이프라인과의 통합__: Kafka → Feature Store → 모델 학습/추론 흐름이 일반화


## 5. Kafka 관리 도구

 - __Kafka Manager (CMAK)__: 토픽, 파티션, 컨슈머 그룹 UI 관리, 무료 사용 가능
 - __Confluent Control Center__: 커넥터, 브로커, 모니터링 통합 UI (유료, Confluent Platform)
 - __Prometheus + Grafana__: Kafka 메트릭 수집 및 대시보드 시각화
 - __Burrow__: Consumer 그룹의 Lag 상태 모니터링 전문 도구
 - __Cruise Control__: 자동 리밸런싱, 파티션 이동, Capacity Planning 기능
 - __Kowl / AKHQ__: Kafka 브로커/토픽/컨슈머를 GUI로 확인, Kafka UI 툴의 현대적인 대안

## 6. Kafka 운영 Best Practies

### 6-1. 토픽 설계 (Topic Design)

 - __파티션 수__: 처음부터 충분한 수로 설계 (일반적으로 Consumer 수 이상). 너무 많아도 브로커 오버헤드 증가.
 - __키 선택__: 키가 잘 분산되도록 설계 (e.g., userId, orderId 등). 쏠림 현상 방지.
 - __보존 정책__: 필요에 따라 log.retention.hours, log.retention.bytes 조정. 컴플라이언스/비즈니스 요건에 따라 다양하게.
 - __압축 설정__: compression.type=snappy 또는 lz4로 설정 → 디스크/네트워크 최적화.

### 6-2. 프로듀서 설정

 - __acks=all__: 모든 ISR에 쓰기 성공해야만 ACK. 데이터 내구성 강화.
 - __retries=5 이상__: 일시적 장애 시 자동 재시도.
 - __linger.ms=5~50__: 배치 크기 증가 → Throughput 향상.
 - __compression.type=snappy__: 네트워크 사용량 감소, 처리량 증가.
 - __키 기반 파티셔닝__: 균등하게 분산되도록 키 설계 (해시 충돌 방지).

### 6-3. 컨슈머 설정

 - __enable.auto.commit=false__: 수동 커밋 사용 → 처리 정확성 확보.
 - __max.poll.records=500~1000__: 한 번에 처리할 메시지 수 조절 → 처리 지연 방지.
 - __병렬 소비__: 파티션 수 ≥ 컨슈머 수 ≥ 처리 스레드 수 권장.
 - __오류 처리__: Dead Letter Queue(DLQ), 재처리 로직 반드시 구축.

### 6-4. 브로커/클러스터 운영 전략

 - __Replication Factor ≥ 3__: 고가용성을 위해 최소 3 이상.
 - __min.insync.replicas 설정__: 2 이상 설정하여 장애 시에도 데이터 유실 방지.
 - __Segment 단위 관리__: log.segment.bytes와 log.retention.* 튜닝으로 스토리지 최적화.
 - __리소스 모니터링__: CPU, Memory, Disk, Network 별로 임계값 설정 후 경보.
 - __JVM 튜닝__: Heap 크기 제한 (-Xmx), GC 모니터링 (G1GC 권장).

### 6-5. 장애 대응 구조

 - __Multi-AZ / Multi-IDC 구성__: 장애 격리 및 가용성 확보 (MirrorMaker, Replicator 사용).
 - __Unclean Leader Election 비활성화__: unclean.leader.election.enable=false → 잘못된 리더 승격 방지.
 - __백업 & 재처리 설계__: 이벤트 기반 시스템은 DLQ 또는 S3 백업 구성 필수.
 - __ISR 모니터링__: ISR(Set of in-sync replicas)이 계속 유지되는지 확인.

### 6-6. 모니터링 및 가시성 확보

 - __Lag 모니터링__: Burrow, Kafka Exporter, Grafana (Consumer Lag = 핵심 지표)
 - __Broker Health__: JVM 메모리, 디스크 사용량, 네트워크 IO 등
 - __토픽 사용량 추이__: 파티션 별 TPS, 리텐션에 따른 증가량 확인
 - __Alert 설정__: ISR 수, under-replicated partitions, consumer lag 등 기준 알람 설정
 - https://grafana.com/grafana/dashboards/?search=kafka

### 6-7. 배포 & 업그레이드 전략

 - __롤링 업그레이드__: 한 번에 하나의 브로커씩 순차적으로 업그레이드
 - __블루-그린 구성__: 대규모 클러스터 업그레이드 시 새로운 클러스터로 전환
 - __버전 호환 확인__: Kafka ↔ Schema Registry ↔ Connect ↔ Streams 간 호환성 필수 체크
