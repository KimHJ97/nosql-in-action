# Kafka Streams

## 1. Kafka Streams 소개

Kafka Streams는 Apache Kafka 위에서 동작하는 Java 기반의 스트리밍 데이터 처리 라이브러리입니다. 복잡한 스트리밍 애플리케이션을 단순한 방식으로 작성할 수 있도록 도와주며, Kafka 클러스터와 자연스럽게 통합됩니다.

Kafka Streams는 일반적인 Kafka 프로듀서/컨슈머처럼 동작하지만, 데이터를 실시간으로 처리하거나 변환, 집계하는 기능을 제공합니다.

 - 라이브러리: 독립적인 클러스터가 아닌, Java 애플리케이션 내에 포함되어 실행됨 (Kafka Streams는 Kafka처럼 별도 서버가 아님)
 - 분산 처리: Kafka의 파티션을 기반으로 자동으로 병렬처리
 - 상태 저장(Stateful): 로컬 상태 저장소(RocksDB 기반)를 이용해 집계, 조인 등을 상태 기반으로 처리
 - Exactly-once 처리 보장 가능

## 2. Kafka Streams 주요 메서드

```java
builder.stream("orders")
    .mapValues(v -> v.toUpperCase())
    .groupByKey()
    .count()
    .toStream()
    .to("order-counts");
```

 - `StreamsBuilder`
    - StreamsBuilder는 Kafka Streams에서 처리 토폴로지를 정의하는 진입점이다.
    - stream(...): 토픽을 KStream으로 읽기
    - table(...): 토픽을 KTable로 읽기 (최신 값 유지)
    - globalTable(...): 전체 파티션을 모든 인스턴스에 복제하는 GlobalKTable 생성
    - build(): 토폴로지를 완성해서 KafkaStreams에 전달
```java
StreamsBuilder builder = new StreamsBuilder();
KStream<String, String> stream = builder.stream("input-topic");
Topology topology = builder.build();
```

 - `KStream`
    - map(...): key와 value 모두 변경
    - mapValues(...): value만 변경
    - filter(...): 조건에 따라 레코드 필터링
    - flatMap(...): 여러 key-value 쌍으로 분해
    - foreach(...): 단순 반복 처리 (side effect 목적)
    - to(...): 결과를 Kafka 토픽에 전송
    - groupByKey(): key를 기준으로 그룹핑
    - groupBy(...): 원하는 기준으로 그룹핑
    - join(...), leftJoin(...): 다른 스트림 또는 테이블과 조인
```java
stream
    .filter((key, value) -> value.contains("hello"))
    .mapValues(value -> value.toUpperCase())
    .to("output-topic");
```

 - `KGroupedStream & KGroupedTable (집계)`
    - count(): 개수 세기
    - reduce(): 누적 처리
    - aggregate(): 복잡한 집계 (초기값 + 누적 로직)
    - windowedBy(...): 윈도우 기반 집계 (시간별 등)
```java
stream.groupByKey()
      .count()
      .toStream()
      .to("count-topic");
```

 - `TimeWindows & SessionWindows (윈도우)`
    - TimeWindows.ofSizeWithNoGrace(...): 고정 윈도우 (grace 없이)
    - SlidingWindows.ofTimeDifference(...): 슬라이딩 윈도우
    - SessionWindows.with(...): 세션 윈도우
```java
stream.groupByKey()
      .windowedBy(TimeWindows.ofSizeWithNoGrace(Duration.ofMinutes(1)))
      .count()
      .toStream()
      .to("windowed-topic");
```

 - `KTable`
    - KTable은 상태 기반으로 최신 값만 유지되는 스트림이다.
    - filter(...): KTable 필터링
    - mapValues(...): 값 변경
    - join(...): 다른 KStream 또는 KTable과 조인
    - toStream(): 스트림으로 변환
```java
KTable<String, String> userTable = builder.table("user-topic");

stream.join(userTable,
    (streamValue, tableValue) -> streamValue + " | " + tableValue)
    .to("joined-topic");
```


## 3. Kafka Streams 사용 예시

 - `docker-compose.yml`
```yml
version: '3'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181

  kafka:
    image: confluentinc/cp-kafka:7.6.0
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
```

 - `build.gradle`
```groovy
dependencies {
    implementation 'org.springframework.boot:spring-boot-starter'
    implementation 'org.apache.kafka:kafka-streams:3.6.0' // Kafka Streams core
    implementation 'org.springframework.kafka:spring-kafka' // Spring-Kafka
    implementation("com.fasterxml.jackson.core:jackson-databind")
}
```

 - `application.yml`
```java
spring:
  kafka:
    bootstrap-servers: localhost:9092
    streams:
      application-id: my-streams-app
      default:
        key-serde: org.apache.kafka.common.serialization.Serdes$StringSerde
        value-serde: org.apache.kafka.common.serialization.Serdes$StringSerde
```

 - `KafkaStreamsConfig`
    - @Bean(name = KafkaStreamsDefaultConfiguration.DEFAULT_STREAMS_CONFIG_BEAN_NAME) 이 이름으로 등록된 Properties Bean을 Spring이 자동으로 Kafka Streams 설정한다.
    - Spring Boot는 이 설정을 바탕으로 StreamsBuilderFactoryBean 을 생성한다. 이 StreamsBuilderFactoryBean이 내부적으로 Kafka Streams의 StreamsBuilder를 생성할 때, 등록된 Properties를 사용한다.
    - StreamsBuilderFactoryBean을 직접 정의하고 싶은 경우 KafkaStreamsConfiguration의 Kafka 서버 설정을 정의하고, KafkaStreamsConfiguration를 매개변수로 StreamsBuilderFactoryBean을 객체화하여 빈으로 등록한다.
```java
@RequiredArgsConstructor
@Configuration
public class KafkaStreamsConfig {

    private final KafkaProperties kafkaProperties;

/*
    @Bean(name = KafkaStreamsDefaultConfiguration.DEFAULT_STREAMS_CONFIG_BEAN_NAME)
    public Properties kafkaStreamsConfigs() {
        Properties props = new Properties();
        props.put(StreamsConfig.APPLICATION_ID_CONFIG, kafkaProperties.getStreams().getApplicationId());
        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaProperties.getBootstrapServers());
        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());
        props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());
        return props;
    }
*/

    @Bean
    public KafkaStreamsConfiguration kafkaStreamsConfiguration() {
        Map<String, Object> props = new HashMap<>();
        props.put(StreamsConfig.APPLICATION_ID_CONFIG, kafkaProperties.getStreams().getApplicationId());
        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaProperties.getBootstrapServers());
        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());
        props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());
        return new KafkaStreamsConfiguration(props);
    }

    @Bean
    public StreamsBuilderFactoryBean streamsBuilderFactoryBean(KafkaStreamsConfiguration configuration) {
        return new StreamsBuilderFactoryBean(configuration);
    }
}

@Getter @Setter
@Component
@ConfigurationProperties(prefix = "spring.kafka")
public class KafkaProperties {

    private String bootstrapServers;
    private Streams streams = new Streams();

    @Getter @Setter
    public static class Streams {
        private String applicationId;
    }
}
```

 - `예시 코드`
    - orders 토픽에서 사용자 주문 이벤트를 읽어서 사용자별 총 주문 수를 집계해서 order-counts 토픽으로 전송
```java
@Configuration
public class KafkaStreamsConfiguration {

    @Bean
    public KStream<String, String> orderStream(StreamsBuilder builder, ObjectMapper objectMapper) {
        KStream<String, String> stream = builder.stream("orders");

        stream
            .map((key, value) -> {
                try {
                    Order order = objectMapper.readValue(value, Order.class);
                    return new KeyValue<>(order.getUserId(), order);
                } catch (Exception e) {
                    return null; // 혹은 로깅 처리
                }
            })
            .filter((userId, order) -> order != null)
            .groupByKey()
            .count()
            .toStream()
            .mapValues((readOnlyKey, count) -> {
                try {
                    OrderCount orderCount = new OrderCount(readOnlyKey, count);
                    return objectMapper.writeValueAsString(orderCount);
                } catch (Exception e) {
                    return null;
                }
            })
            .to("order-counts", Produced.with(Serdes.String(), Serdes.String()));

        return stream;
    }

    @Bean
    public ObjectMapper objectMapper() {
        return new ObjectMapper();
    }
}
```

 - `테스트`
```bash
# orders 토픽에 JSON 메시지 전송
echo '{"userId": "user1", "amount": 100}' | \
kafka-console-producer.sh --broker-list localhost:9092 --topic orders

# order-counts 토픽에서 확인
kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic order-counts --from-beginning
```
