# KSQL 실습

## 1. KSQL 설치

```bash
vi /home/ubuntu/confluent-7.5.2/etc/ksqldb/ksql-server.properties
# bootstrap.servers 변경

# KSQL 서버 실행
cd /home/ubuntu/confluent-7.5.2/bin
./sql-server-start /home/ubuntu/confluent-7.5.2/etc/ksqldb/ksql-server.properties &
```

## 2. 기본적 KSQL Query와 Object별 특징 실습

 - `Stream Object`: 연속적인 record의 흐름, 집계보다 데이터 자체를 담는 경우가 많음
 - `Table Object`: 최신 상태를 보여주는 데이터를 넣는 경우가 많음. key-value 형식으로 구성되어 있음.

### 2-1. Ksql Query

 - `Push Query`
    - 데이터가 변화할 때마다 결과를 반환
    - 실시간으로 변경되는 데이터의 파악이 필요할 때
 - `Pull Query`
    - 실행하는 시점의 데이터를 반환
    - 현재 상태에 대해 파악이 필요할 때


### 2-2. Ksql DataType

 - `Primitive Types (기본 데이터 타입)`
    - BOOLEAN: 참 또는 거짓 값을 나타내는 불리언 타입.
    - INTEGER: 정수 값을 나타내는 타입.
    - BIGINT: 더 큰 범위의 정수 값을 나타내는 타입.
    - DOUBLE: 부동 소수점 숫자를 나타내는 타입.
    - STRING: 문자열을 나타내는 타입.
    - BYTES: 바이트 배열을 나타내는 타입.
 - `Complex Types (복합 데이터 타입)`
    - ARRAY: 동일한 타입의 여러 값들을 순서대로 나열한 배열 타입.
    - MAP: 키-값 쌍을 갖는 맵 타입.
    - STRUCT: 여러 필드를 가지는 구조체 타입. 각 필드에는 이름과 타입이 정의됩니다.
 - `Special Types (특수 데이터 타입)`
    - TIMESTAMP: 날짜와 시간을 나타내는 타입. 일반적으로 '1970-01-01 00:00:00 UTC' 이후의 밀리초로 표현됩니다.
    - DATE: 날짜만을 나타내는 타입.
    - TIME: 시간만을 나타내는 타입.
    - DECIMAL(precision, scale): 정밀한 소수점을 가진 숫자를 나타내는 타입. precision은 숫자의 총 자릿수를, scale은 소수점 이하의 자릿수를 나타냅니다.

### 2-3. Sample Query Test

 - Kafka UI 작업
    - Topic 생성
    - Stream 생성
    - Sample Data 발행
    - Stream 조회
    - Sample Data 재발행
    - Stream Join 및 결과 조회
    - Table 생성 및 조회

#### 샘플 데이터 및 쿼리

 - `샘플 데이터 (KSql_Test_Data.txt)`
```txt
// login_topic sample data
{"loginDt": "2024-01-16T08:00:00", "user_id": 1, "username": "user1"}
{"loginDt": "2024-01-16T09:30:00", "user_id": 2, "username": "user2"}
{"loginDt": "2024-01-16T10:45:00", "user_id": 3, "username": "user3"}

// logout_topic sample data
{"logout_dt": "2024-01-16T10:15:00", "user_id": 1}
{"logout_dt": "2024-01-16T12:00:00", "user_id": 2}
{"logout_dt": "2024-01-16T13:30:00", "user_id": 3}
```

 - `ksql_query_test.txt`
```sql
-- stream creation
CREATE STREAM login_info_share (
  login_dt string, user_id string, username string
) WITH (kafka_topic = 'loginTopic', VALUE_FORMAT = 'JSON');


CREATE STREAM logout_info_share
(
  logout_dt string, user_id string, username string
) WITH (KAFKA_TOPIC='logout', VALUE_FORMAT='JSON');

select unix_timestamp(logout_dt) / 1000, user_id
from logout_info_share;

select a.login_dt, b.logout_dt, a.user_id, a.username
from login_info_share a
left join logout_info_share b WITHIN 5 HOURS ON a.user_id = b.user_id
emit changes;

-- create stream-join (5hrs, 1min)
create stream session_info_share_5hrs as
select a.login_dt, b.logout_dt, a.user_id, a.username
from LOGIN_INFO_SHARE a
inner join logout_info_share b within 1 minutes on a.user_id = b.user_id
emit changes;

create stream session_info_share_1min as
select a.login_dt, b.logout_dt, a.user_id, a.username
from LOGIN_INFO_SHARE a
inner join logout_info_share b within 1 minutes on a.user_id = b.user_id
emit changes;

select user_id, username, (unix_timestamp(logout_dt) - unix_timestamp(login_dt)) / 1000 as session_age
from session_info_share;

-- create table-avg
create table avg_session_age_info AS
select a_user_id, avg((unix_timestamp(logout_dt)- unix_timestamp(login_dt)) /1000) as avg_session_age
from session_info_share_1min
group by a_user_id;
```

## 3. KSQL 활용을 통한 이상 Transaction 감지

### 3-1. WebLog 기반의 이상 반복 호출 감지

 - WebLogTopic 활용
 - access_policy 생성 및 sample data 입력
 - access stream, counting table 생성 및 조회
 - stream-table Join
 - block_list stream 생성 및 조회


 - `쿼리 (ksql_query.txt)`
```sql
-- Step 1: 기본 스트림 생성
CREATE STREAM fs_access_log  (
    policy_type string key,
    logdate STRING,	
    loglevel STRING,
    `ub.ip_address` STRING,
    `ub.page_url` STRING,
    `ub.referrer` STRING,
    `ub.response_code` INTEGER,
    `ub.session_id` STRING,
    `ub.timestamp` STRING,
    `ub.user_agent` STRING,
    `ub.user_event` STRING
) WITH (KAFKA_TOPIC='webLogPulseSourceConnector-16', VALUE_FORMAT='JSON');

-- Step 2:  기본 stream 에서 alias 변경
CREATE STREAM fs_access_log_simple AS
    SELECT
        policy_type,
        logdate,
        loglevel,
        `ub.ip_address` AS ip_address,
        `ub.page_url` AS page_url,
        `ub.referrer` AS referrer,
        `ub.response_code` AS response_code,
        `ub.session_id` AS session_id,
        STRINGTOTIMESTAMP(`ub.timestamp`, 'yyyy-MM-dd''T''HH:mm:ssX') AS ub_timestamp,
        `ub.user_agent` AS user_agent,
        `ub.user_event` AS user_event
    FROM fs_access_log

-- Step 3 : 사용대상 stream에서 10초 단위로 데이터 변경
create STREAM fs_access_log_10s_term as
  select policy_type, ip_address, (ub_timestamp/1000)-(ub_timestamp/1000%10) fr_dt, (ub_timestamp/1000)+(10-ub_timestamp/1000%10) as to_dt
  from fs_access_log_simple;

-- Step 4 : policy stream 생성
create STREAM fs_abnormal_tx_policy (
  policy_id string,
  policy_type string,
  policy_limit integer,
  standard integer,
  unit string,
  created_time string
)
with(kafka_topic='fs_abnormal_tx_policy', value_format = 'json');

-- Step 5 : policy grouping - type1
CREATE TABLE current_policy as
SELECT
    policy_type,
    LATEST_BY_OFFSET(policy_id) AS latest_policy_id,
    LATEST_BY_OFFSET(standard) AS latest_standard,
    LATEST_BY_OFFSET(unit) AS latest_unit,
    LATEST_BY_OFFSET(created_time) AS latest_created_time,
    LATEST_BY_OFFSET(policy_limit) AS policy_limit
FROM
    fs_abnormal_tx_policy
GROUP BY
    policy_type
EMIT CHANGES;

-- Step 5 : policy grouping - type2
CREATE TABLE wd_current_policy as
select policy_type, max(policy_limit) as limit
from fs_abnormal_tx_policy
WINDOW TUMBLING (SIZE 10 seconds, RETENTION 2 DAYS)
group by policy_type;

-- Step 6: 임계값 초과 여부 확인
select a.policy_type, b.policy_limit, ip_address, from_unixtime(fr_dt*1000) fr_dt, from_unixtime(to_dt*1000) to_dt, count(*) access_cnt 
from fs_access_log_10s_term a left join fs_policy_offset b on a.policy_type = b.policy_type
group by a.policy_type, b.policy_limit, ip_address,  from_unixtime(fr_dt*1000), from_unixtime(to_dt*1000)
having count(*) > b.policy_limit
emit changes;
```
