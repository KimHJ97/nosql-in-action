# Mirrormaker2

## 1. Mirrormaker2소개 및 구성실습

MirrorMaker2 (MM2)는 Apache Kafka의 공식 도구로, 여러 Kafka 클러스터 간에 데이터를 복제(Mirroring) 할 수 있게 해줍니다. MM2는 Kafka Connect 프레임워크 위에 구축되어 있으며, Cross-Cluster 데이터 동기화, 다중 데이터 센터 환경, 클라우드-온프레미스 하이브리드 구조 같은 시나리오에서 유용하게 쓰입니다.

 - Kafka를 MultiCluster로 이용할 때, 여러 목적으로 Cluster 간의 데이터 이동이 필요할 떄 사용하는 도구
 - standalone Mode에서는 독립적으로 작동하며, connect를 이용해서 distributed Mode로 실행되면 하나의 플러그인으로 동작

### 1-1. MirrorMaker2 사용 사례

 - `멀티 데이터센터 운영 (Active-Passive / Active-Active)`
    - 목적: 장애 복구(Disaster Recovery), 고가용성 확보
    - 설명: 한 데이터센터에서 장애가 발생하면 다른 데이터센터로 자동 전환할 수 있도록 Kafka 데이터를 미리 복제해둠.
    - 예시:
        - AIDC (Active-Inactive): 한국에 메인 클러스터, 미국에 백업 클러스터 → 장애 발생 시 미국 클러스터로 전환
        - AADC (Active-Active): 두 클러스터 모두 동작 중, 데이터를 양방향으로 동기화
 - `클라우드 마이그레이션`
    - 목적: 온프레미스 Kafka → 클라우드 Kafka 이전 시 데이터 손실 없이 마이그레이션
    - 설명: MirrorMaker2를 통해 기존 클러스터의 데이터를 클라우드 Kafka로 실시간 복제
    - 예시: AWS MSK, Confluent Cloud로 Kafka 이전을 고려하는 기업이 자주 사용. 마이그레이션 기간 동안 두 클러스터 병렬 운영 가능
 - `지역별 Kafka 클러스터 분산 운영(다중 리전 애플리케이션)`
    - 목적: 사용자 지연 최소화, 데이터 지역화
    - 설명: 아시아, 유럽, 북미 등 지역별 Kafka 클러스터를 운영하면서 일부 핵심 데이터를 서로 복제
    - 예시: 글로벌 서비스 기업(예: Netflix, Uber): 사용자의 위치에 따라 가까운 클러스터에서 처리하되, 로그나 사용자 이벤트는 중앙 클러스터에 복제
 - `하이브리드 아키텍처 (On-Prem ↔ Cloud)`
    - 목적: 일부 워크로드는 클라우드에서 처리하고, 중요한 데이터는 온프레미스에 저장
    - 설명: MirrorMaker2로 데이터를 양방향으로 복제하여 하이브리드 환경 구성
    - 예시: 클라우드 기반 AI 모델이 실시간 데이터를 처리하고 결과를 온프레미스 분석 시스템에 저장

### 1-2. 예시

 - `mm2-test.json`
```json
{
  "name":"fc-mm2-test",
  "config": {
    "connector.class": "org.apache.kafka.connect.mirror.MirrorSourceConnector",
    "name":"fc-mm2-test3",
    "source.cluster.alias":"source",
    "topics":"debe-avro-source-.public.fast_jdbc_test",
    "source.cluster.bootstrap.servers":"IP:9092",
    "target.cluster.bootstrap.servers":"IP:9092",
    "producer.override.bootstrap.servers":"IP:9092",
    "offset-syncs.topic.replication.factor":"1"
  }
}
```

 - `mm2-test-transform.json`
```json
{
  "name":"fc-mm2-trsf2",
  "config": {
    "connector.class": "org.apache.kafka.connect.mirror.MirrorSourceConnector",
    "name":"fc-mm2-trsf2",
    "source.cluster.alias":"source",
    "topics":"debe-avro-source-.public.fast_jdbc_test",
    "source.cluster.bootstrap.servers":"IP:9092",
    "target.cluster.bootstrap.servers":"IP:9092",
    "producer.override.bootstrap.servers":"IP:9092",
    "offset-syncs.topic.replication.factor":"1",
    "key.converter": "org.apache.kafka.connect.converters.ByteArrayConverter",
    "value.converter": "org.apache.kafka.connect.converters.ByteArrayConverter"
  }
}
```

## 2. Connect 예제 (사용자 behavior log의 분석을 위한 데이터 이동)

### 2-1. 실습 준비

 - filePulseSourceConnector: https://github.com/streamthoughts/kafka-connect-file-pulse
 - elasticsearchSinkConnector: https://github.com/confluentinc/kafka-connect-elasticsearch/tree/master
```bash
sudo mkdiro /user/share/java/connectors
ssh–i yourkey.pem ubuntu@yourPublicIp
scp–i yourkey.pem /yourLibPath/* ubuntu@yourPublicIp:/yourTargetPath/
sudo mv /yourTargetPath/*  /usr/share/java/connectors # (filePulse, ES 2번 수행)
sudo service connect restart
git에서 weblog.log download 
mkdir /home/ubuntu/weblog
```

### 2-2. SourceConnector (File -> Kafka)

 - `filePulseSourceConnector.json`
```json
{
  "name": "fc-log4j-filepulse-source",
  "config": {
    "connector.class": "io.streamthoughts.kafka.connect.filepulse.source.FilePulseSourceConnector",
    "filters": "GroupMultilineException, ParseLog4jLog",
    "filters.GroupMultilineException.negate": "false",
    "filters.GroupMultilineException.pattern": "^[\\t]",
    "filters.GroupMultilineException.type": "io.streamthoughts.kafka.connect.filepulse.filter.MultiRowFilter",
    "filters.ParseLog4jLog.pattern": "%{TIMESTAMP_ISO8601:logdate} %{LOGLEVEL:loglevel} %{GREEDYDATA:message}",
    "filters.ParseLog4jLog.overwrite": "message",
    "filters.ParseLog4jLog.source": "message",
    "filters.ParseLog4jLog.type": "io.streamthoughts.kafka.connect.filepulse.filter.GrokFilter",
    "filters.ParseLog4jLog.ignoreFailure": "true",
    "fs.cleanup.policy.class": "io.streamthoughts.kafka.connect.filepulse.fs.clean.LogCleanupPolicy",
    "fs.listing.class" : "io.streamthoughts.kafka.connect.filepulse.fs.LocalFSDirectoryListing",
    "fs.listing.directory.path" : "/home/ubuntu/weblog/",
    "fs.listing.filters": "io.streamthoughts.kafka.connect.filepulse.fs.filter.RegexFileListFilter",
    "fs.listing.interval.ms": "10000",
    "file.filter.regex.pattern": "weblog.log",
    "tasks.file.status.storage.bootstrap.servers" : "IP:9092",
    "offset.strategy": "name",
    "read.max.wait.ms": "5000",
    "tasks.reader.class": "io.streamthoughts.kafka.connect.filepulse.fs.reader.LocalRowFileInputReader",
    "topic": "fc-log4j-filepulse-source",
    "tasks.max": 1,
    "key.converter": "org.apache.kafka.connect.json.JsonConverter",
    "value.converter": "org.apache.kafka.connect.json.JsonConverter",
    "key.converter.schemas.enable": "false",
    "value.converter.schemas.enable": "false"
 }
}
```

 - `filePulseSourceConnector-flatten.json`
    - JSON 문자열로 저장된 message를 풀어서 저장
```json
{
  "name": "fc-log4j-filepulse-source-flatten",
  "config": {
    "connector.class": "io.streamthoughts.kafka.connect.filepulse.source.FilePulseSourceConnector",
    "filters": "GroupMultilineException, ParseLog4jLog, MyJsonFilter",
    "filters.GroupMultilineException.negate": "false",
    "filters.GroupMultilineException.pattern": "^[\\t]",
    "filters.GroupMultilineException.type": "io.streamthoughts.kafka.connect.filepulse.filter.MultiRowFilter",
    "filters.ParseLog4jLog.pattern": "%{TIMESTAMP_ISO8601:logdate} %{LOGLEVEL:loglevel} %{GREEDYDATA:message}",
    "filters.ParseLog4jLog.overwrite": "message",
    "filters.ParseLog4jLog.source": "message",
    "filters.ParseLog4jLog.type": "io.streamthoughts.kafka.connect.filepulse.filter.GrokFilter",
    "filters.ParseLog4jLog.ignoreFailure": "true",
    "fs.cleanup.policy.class": "io.streamthoughts.kafka.connect.filepulse.fs.clean.LogCleanupPolicy",
    "fs.listing.class" : "io.streamthoughts.kafka.connect.filepulse.fs.LocalFSDirectoryListing",
    "fs.listing.directory.path" : "/home/ubuntu/weblog/",
    "fs.listing.filters": "io.streamthoughts.kafka.connect.filepulse.fs.filter.RegexFileListFilter",
    "fs.listing.interval.ms": "10000",
    "file.filter.regex.pattern": "weblog.log",
    "tasks.file.status.storage.bootstrap.servers" : "IP:9092",
    "offset.strategy": "name",
    "read.max.wait.ms": "5000",
    "tasks.reader.class": "io.streamthoughts.kafka.connect.filepulse.fs.reader.LocalRowFileInputReader",
    "topic": "fc-log4j-filepulse-source",
    "tasks.max": 1,
    "key.converter": "org.apache.kafka.connect.json.JsonConverter",
    "value.converter": "org.apache.kafka.connect.json.JsonConverter",
    "key.converter.schemas.enable": "false",
    "value.converter.schemas.enable": "false",
    "filters.MyJsonFilter.type" : "io.streamthoughts.kafka.connect.filepulse.filter.JSONFilter",
    "filters.MyJsonFilter.source" : "message",
    "filters.MyJsonFilter.target" : "ub",
    "transforms": "removeMessageField, flatten",
    "transforms.removeMessageField.type": "org.apache.kafka.connect.transforms.ReplaceField$Value",
    "transforms.removeMessageField.blacklist": "message",
    "transforms.flatten.type": "org.apache.kafka.connect.transforms.Flatten$Value",
    "transforms.flatten.fields" : "contents"
  }
}
```

 - `Connector 생성`
```bash
# Connector 생성
curl -X POST -H "Content-Type: application/json" --data @filePulseSourceConnector.json http://localhost:8083/connectors

# Connector 확인
curl -X GET http://localhost:8083/connectors/fc-log4j-filepulse-source/status
```

### 2-3. SinkConnector (Kafka -> ElasticSearch)

 - `ElasticSearch 설치`
```bash
sudo apt-get update
sudo apt-get install docker.io
docker network create elastic
docker run --name es01-test -d --net elastic -p 9200:9200 -p 9300:9300 -e "discovery.type=single-node" docker.elastic.co/elasticsearch/elasticsearch:7.17.16
docker run --name kib01-test -d --net elastic -p 5601:5601 -e "ELASTICSEARCH_HOSTS=http://IP:9200" docker.elastic.co/kibana/kibana:7.17.16

# 설치 확인
# Browser에서 publicIP:5601 로 접근
curl –X GET http://localhost:9200
```

 - `webLogSinkConnecctor.json`
```json
{
  "name": "webLogEsSinkConnector-16",
  "config": {
    "connector.class": "io.confluent.connect.elasticsearch.ElasticsearchSinkConnector",
    "tasks.max": "1",
    "topics": "fc-log4j-filepulse-source",
    "key.ignore": "true",
    "schema.ignore": "true",
    "connection.url": "http://IP:9200",
    "type.name": "_doc",
    "index.name" : "webLogPulseSourceConnector-16",
    "connection.username": "",
    "connection.password": "",
    "key.converter": "org.apache.kafka.connect.json.JsonConverter",
    "value.converter": "org.apache.kafka.connect.json.JsonConverter",
    "key.converter.schemas.enable": "false",
    "value.converter.schemas.enable": "false"
  }
}
```

 - `webLotSinkConnector-json.json`
```json
{
  "name": "weblogToEs5",
  "config": {
    "connector.class": "io.confluent.connect.elasticsearch.ElasticsearchSinkConnector",
    "tasks.max": "1",
    "topics": "FilePulseSourceWithoutSR",
    "key.ignore": "true",
    "schema.ignore": "true",
    "connection.url": "http://13.125.10.180:9200",
    "type.name": "_doc",
    "connection.username": "",
    "connection.password": "",
    "key.converter": "org.apache.kafka.connect.json.JsonConverter",
    "value.converter": "org.apache.kafka.connect.json.JsonConverter",
    "key.converter.schemas.enable": "false",
    "value.converter.schemas.enable": "false"
  }
}
```

 - `Connector 생성`
```bash
# Connector 생성
curl -X POST -H "Content-Type: application/json" --data @webLogSinkConnector.json http://localhost:8083/connectors

# Connector 확인
curl -X GET http://localhost:8083/connectors/webLogSinkConnector/status
```
